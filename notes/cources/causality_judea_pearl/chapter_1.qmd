---
title: Chapter 1. Introduction to Probabilities, Graphs, and Causal Models
description: Вводная глава о базовых концепциях
date: "2025-02-18"
categories: [cources]
---

# Вероятность

Аксиомы:  
* $0 <= P(A) <= 1$  
* $P(True) = 1$   
* $P(A\ or\ B) = P(A) + P(B)$, если события $A$ и $B$ взаимоисключащие     

**Закон полной вероятности** - так как любое событие $A$ можно описать как объединение событий $(A \wedge B)$ и $(A \wedge \neg B)$, следовательно вероятность события описывается формулой $P(A) = P(A \wedge B) + P(A \wedge \neg B)$, тогда если $B_i$ это множество исчерпывающих и взаимоисключающих состояний, то вероятность $P(A \wedge B)$ можно посчитать как сумму $P(A)=\sum_{i} P(A \wedge B_i)$

Из фактов выше следует что $P(A) + P(\neg A) = 1$ потому что хотя бы одно из событий должно быть истинным. 

**Условная вероятность** - $P(A|B)$ - вероятность события $A$ если задано событие $B$  

**Независимость** - $A$ и $B$ назвисисимы, если $P(A|B)=P(A)$, если $P(A|B \wedge C)=P(A|C)$, то $A$ и $B$ условно независимы при заданноом событии $C$


**Формула условной вероятности** - описывает условную вероятность совместных событий $P(A,B)=P(A|B) * P(B)$ если что $P(A,B)$ это то же самое что и $P(A\wedge B)$. Можно расширить ее с помощью закона полной вероятности, ведь вероятность события $A$ сумме равна сумме условной вероятнотсти события $A$ для всех взаимно исключащих и исчерпывающих $B$ т.е. $P(A)=\sum_{i} P(A|B_i)P(B_i)$.

**Формула Баеса**  - $P(H|e)=\frac{P(e|H)*P(H)}{P(e)}$, она так популярна, что у каждого компонента даже название есть: 
- $P(H|e)$ - апостериорная вероятность события (постериор) - вероятность $H$ зная что случилось $e$
- $P(e|H)$ - правдоподобие 'likelihood' - вероятность $e$ если $H$ произошло
- $P(H)$ - априорная вероятность события (приор)
- $P(e)$ - коэффициент нормализации

Эту формулу можно искусственно поделить на $P(\neg H|e)$, чтобы получить занятные характеристики. 
Целиком формула станет выглядеть так $\frac{P(H|e)}{P(\neg H|e)}=\frac{P(e|H)}{P(e|\neg H)} * \frac{P(H)}{P(\neg H)}$, где:   
- $O(H) = \frac{P(H)}{P(\neg H)} = \frac{P(H)}{1 - P(\neg H)}$ - априорный шанс - прогнозная степень уверенности в событии H основанная только на предварительных данных 
- $L(e|H) = \frac{P(e|H)}{P(e|\neg H)}$ - отношение правдоподобия - ретроспективная оценка вероятности события H дополненная наблюдаемыми данными 
- $O(H|e)=\frac{P(H|e)}{P(\neg H|e)}$ - постериорный шанс 
Тогда все можно переписать в виде $O(H|e) = L(e|H)*O(H)$

**Вероятностноая модель / вероятностное пространство** - Это определение случайного процесса. В нем важны: 
- множество элементарных событий 
- совокупность возможных взимосвязей элементарных событий выраженная простейшими логическими операциями 
- функция распределения - конструкция оценивающая не отрицательные значения вероятностей для каждой комбинации, сумма которых дает 1  

**Условная независимость** - пусть $V={V_1, V_2, ...}$ конечное множество переменных. Пусть $P(\cdot)$ это функция совметной вероятности для переменных $V$, также пусть $X,Y,Z$ любые три подмножества переменных $V$, тогда множества $X$ и $Y$ условно независимы при заданном $Z$ если $P(x|y, z)=P(x|z)$ если $P(y,z)>0$. Это можно описать как то, что знание $Y$ не дает никакой дополнительной ифнормации об $X$ если известно $Z$. 

**graphoid axioms** - условная независимость обозначается как $(X \perp Y | Z)$ и имеет следующие свойства:   
* симметрия $(X \perp Y | Z) \implies (Y \perp X | Z)$ - если при известном Z, Y не говорит ничего нового об X, то и X не говорит ничего нового об Y  
* декомпозиция $(X \perp YW| Z) \implies (X \perp Y| Z)$ - если 2 объединенных факта YW не важны для X то они не важны и по отдельности  
* слабое объединение $(X \perp YW| Z) \implies (X \perp Y|ZW)$ - получение нерелевантной информации W не позволяет сделать Y релевантным для X   
* сокражение $(X \perp Y | Z) \& (X \perp W |ZY) \implies (X \perp YW | Z)$ - если при получении данных W выясняется что Y не релевантен X, то и до получения этих данных он должен быть нерелевантен   
* пересечение $(X \perp W | ZY) \& (X \perp Y|ZW) \implies (X \perp YW|Z)$ - если W не важен для X при известном Y и Y не важен для X при известном W то ни W ни Y ни их комбинации не важны для X  









