---
title: Chapter 1. Introduction to Probabilities, Graphs, and Causal Models
description: Вводная глава о базовых концепциях
date: "2025-02-18"
categories: [cources]
---

# Вероятность

Аксиомы:  
* $0 <= P(A) <= 1$  
* $P(True) = 1$   
* $P(A\ or\ B) = P(A) + P(B)$, если события $A$ и $B$ взаимоисключащие     

**Закон полной вероятности** - так как любое событие $A$ можно описать как объединение событий $(A \wedge B)$ и $(A \wedge \neg B)$, следовательно вероятность события описывается формулой $P(A) = P(A \wedge B) + P(A \wedge \neg B)$, тогда если $B_i$ это множество исчерпывающих и взаимоисключающих состояний, то вероятность $P(A \wedge B)$ можно посчитать как сумму $P(A)=\sum_{i} P(A \wedge B_i)$

Из фактов выше следует что $P(A) + P(\neg A) = 1$ потому что хотя бы одно из событий должно быть истинным. 

**Условная вероятность** - $P(A|B)$ - вероятность события $A$ если задано событие $B$  

**Независимость** - $A$ и $B$ назвисисимы, если $P(A|B)=P(A)$, если $P(A|B \wedge C)=P(A|C)$, то $A$ и $B$ условно независимы при заданноом событии $C$


**Формула условной вероятности** - описывает условную вероятность совместных событий $P(A,B)=P(A|B) * P(B)$ если что $P(A,B)$ это то же самое что и $P(A\wedge B)$. Можно расширить ее с помощью закона полной вероятности, ведь вероятность события $A$ сумме равна сумме условной вероятнотсти события $A$ для всех взаимно исключащих и исчерпывающих $B$ т.е. $P(A)=\sum_{i} P(A|B_i)P(B_i)$.

**Формула Баеса**  - $P(H|e)=\frac{P(e|H)*P(H)}{P(e)}$, она так популярна, что у каждого компонента даже название есть: 
- $P(H|e)$ - апостериорная вероятность события (постериор) - вероятность $H$ зная что случилось $e$
- $P(e|H)$ - правдоподобие 'likelihood' - вероятность $e$ если $H$ произошло
- $P(H)$ - априорная вероятность события (приор)
- $P(e)$ - коэффициент нормализации

Эту формулу можно искусственно поделить на $P(\neg H|e)$, чтобы получить занятные характеристики. 
Целиком формула станет выглядеть так $\frac{P(H|e)}{P(\neg H|e)}=\frac{P(e|H)}{P(e|\neg H)} * \frac{P(H)}{P(\neg H)}$, где:   
- $O(H) = \frac{P(H)}{P(\neg H)} = \frac{P(H)}{1 - P(\neg H)}$ - априорный шанс - прогнозная степень уверенности в событии H основанная только на предварительных данных 
- $L(e|H) = \frac{P(e|H)}{P(e|\neg H)}$ - отношение правдоподобия - ретроспективная оценка вероятности события H дополненная наблюдаемыми данными 
- $O(H|e)=\frac{P(H|e)}{P(\neg H|e)}$ - постериорный шанс 
Тогда все можно переписать в виде $O(H|e) = L(e|H)*O(H)$

**Вероятностноая модель / вероятностное пространство** - Это определение случайного процесса. В нем важны: 
- множество элементарных событий 
- совокупность возможных взимосвязей элементарных событий выраженная простейшими логическими операциями 
- функция распределения - конструкция оценивающая не отрицательные значения вероятностей для каждой комбинации, сумма которых дает 1  

**Условная независимость** - пусть $V={V_1, V_2, ...}$ конечное множество переменных. Пусть $P(\cdot)$ это функция совметной вероятности для переменных $V$, также пусть $X,Y,Z$ любые три подмножества переменных $V$, тогда множества $X$ и $Y$ условно независимы при заданном $Z$ если $P(x|y, z)=P(x|z)$ если $P(y,z)>0$. Это можно описать как то, что знание $Y$ не дает никакой дополнительной ифнормации об $X$ если известно $Z$. 

**graphoid axioms** - условная независимость обозначается как $(X \perp Y | Z)$ и имеет следующие свойства:   
* симметрия $(X \perp Y | Z) \implies (Y \perp X | Z)$ - если при известном Z, Y не говорит ничего нового об X, то и X не говорит ничего нового об Y  
* декомпозиция $(X \perp YW| Z) \implies (X \perp Y| Z)$ - если 2 объединенных факта YW не важны для X то они не важны и по отдельности  
* слабое объединение $(X \perp YW| Z) \implies (X \perp Y|ZW)$ - получение нерелевантной информации W не позволяет сделать Y релевантным для X   
* сокращение $(X \perp Y | Z) \& (X \perp W |ZY) \implies (X \perp YW | Z)$ - если при получении данных W выясняется что Y не релевантен X, то и до получения этих данных он должен быть нерелевантен   
* пересечение $(X \perp W | ZY) \& (X \perp Y|ZW) \implies (X \perp YW|Z)$ - если W не важен для X при известном Y и Y не важен для X при известном W то ни W ни Y ни их комбинации не важны для X  


# Графы и вероятность

Граф состоит из $V$ вершин (нод) и множества $E$ ребер соединяющих пары першин. Две соединенные ребром вершины называются соседними.
Граф может быть ориентированным, если его ребрам присовено направление. Цикл в графе это возможность "пройти по стрелочкам" так, чтобы вернуться в ту вершину, из которой начался путь. 
Направленный граф без циклов называется DAG - directed acyclig graph. 

**Марковские родители** - множество переменных $PA_i$ называется Марковскими родителями для $X_j$ если $PA_i$ это минимальное подмножество предшествующих нод для 
$X_j$ если они делают $X_j$ независимыми от всех остальных предшественников. 

**Марковская совместимость** - если вероятностная функция $P$ согласуется с разложением направленного ациклического графа $G$, то можно сказать, что
$G$ совместим с $P$

**d-разделимость** - путь $p$ называется $d$-разделимым (заблокированным) множеством вершин $Z$ если:   
* $p$ содержит цень $i->m->j$ или ветвление $i<-m->j$ такое, что вершина $m$ находится в $Z$  
* $p$ содержит обратное ветвление (коллайдер) $i->m<-j$ и вершина $m$ не находится в $Z$ и ни один потомок $m$ также не находится в $Z$  

Множество $Z$ $d$-разделяет путь $X->Y$ если $Z$ блокирует каждый путь от $X$ к $Y$

**Теорема: Вероятностный смысл d-разделимости** - если множества $X$ и $Y$ $d$-разделимы по $Z$ в направленном ациклическом графе $G$, то $X$
условно не зависит от $Y$ при заданных $Z$ при любом распределении совместимом с $G$

# Причинные баесовские сети

**Баесовская причинная сеть** - Пусть $P(v)$ вероятностное распределение для множества переменных $V$, и пусть $P_x(v)$ обозначает распределение полученное
в результате воздействия $do(X=x)$ которое приравнивает подмножество переменных $X$ к константам $x$. Обозначим как $P*$ множество всех 
воздействий $P_x(v), X \subseteq V$ включая $P(v)$, которое отображает отсутствие воздействий. Тогда DAG G можно назвать причинной Баесовской сетью 
совместимой с $P*$ тогда и только тогда, если для каждого $P_x \in P*$ выполняется:   
* $P_x(v)$ сопоставима с G    
* $P_x(v_i)=1$ для всех $V_i \in X$ когда $v_i$ согласуется с $X = x$  
* $P_x(v_i|pa_i)=P(v_i|pa_i)$ для всех  $V_i \notin X$ когда $pa_i$ согласуется с $X = x$. т.е. каждый $P(v_i|pa_i)$ остается неизменен от воздействий не связанных с $V_i$  

Свойства: 
* для всеx $i$ $P(v_i|pa_i)=P_{pa_i}(v_i)$
* для всех $i$ и каждого подмножества $S$ переменных непересекающхся с $\{V_i, PA_i\}$ выполняется $P_{pa_i,s}(v_i)=P_{pa_i}(v_i)$

# Причинные функциональные модели

В описанной выше Баесовской концепции все связи предполагаются строго стохастическими, следовательно описываемые системы 
сугубо вероятностны а детерминированное описание это соглашение для удобства работы с такими системами. В противоположность 
Лаплас описывает свой детерминизм, согласно которому все законы строго опредеелны, а случайность возникает из-за того, что
какие-то переменные остаются ненаблюдаемыми.

В книге (напомню что это конспект) - отдается предпочтение квази-детерминизму Лапласа, по трем причинам:   
* Это более общая концепция, каждая случайная система может быть описана множеством функциональных связей, но обратное неверно  
* Эта концепция проще для человеческого восприятия   
* Эта парадигма позволяет описать контрфакты и причнность, то есть рассуждения вида "вероятность события B **из-за** наступления события A" 

В общем виде функциональная причинная модель состоит из множества уровнений вида $x_i=\mathcal{f}(pa_i, u_i), i=1,...,n$, где:   
* $pa_i$ - множество переменных, которые напрямую влияют на $X_i$  
* $U_i$ - ошибки в оценках из-за пропущенных факторов   
Это нелинейное и непараметрическое обобщение линейных структурных моделей (SEM): $x_i=\sum_{k\neq1}(a_{ik}x_k + u_i)$ 

Есть 3 типа запросов, на которые хотим отвечать своими моделями:   
* предсказания - что будет с Y, если мы узнаем, что X произошло   
* воздействие - что будет с Y, если мы заставим X случиться   
* контрфакт - что будет с Y, если X случится, хотя нам известно, что когда X не случается Y=2  

**Условие причинности Маркова** - в корректной Марковской причинной модели каждая переменная $X_i$ независима от ее не потомков
при заданных родительских нодах $PA_i$. Это накладывает 2 предпосылки:   
* полнота - мы должны включить в модель все переменные, которые влияют на 2 или больше переменных    
* общая причина - если 2 переменных зависимы, то либо одна является причиной другой, либо есть третья, которая определяет обе предыдущие    

Есть 3 шага, чтобы для любой причинной модели $M$ при известном наблюдении $e$ получить вероятность $Y=y$ при заданных предполагаемых условиях $X=x$
1) Абдукция - обновить вероятность, чтобы получить $P(u|e)$ - объясняем прошлое с учетом поулченных знаний $e$
2) Действие - приравнять фичи из множества $X=x$ - изменяем наблюдаемые параметры, на те, которые хотим изучть 
3) Предсказание - получить оценку $Y=y$ от модели - прдсказываем будущее с новым пониманием прошлого и наложенными теоретическими условиями 

И наконец, чем причинные модели отличаются от статистических?   
За каждой причинной моделью лежат причинные предпосылки, которые не могут заметны из совместного распрделения6 а следовательно и не могут быть протестированы описательным исследованием. 




