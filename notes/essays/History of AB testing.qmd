---
title: History of AB
description: Видение того, как развивались и будут развиваться рандомизированные тесты
date: "2024-07-31"
categories: [essay, AB testing, statistics]
---

Вот читаю современные статьи от каких-нибудь microsoft research или других крутых ребят и поражаюсь как далеко люди заходят в попытке ответить на вопросы из сферы причин, следствий и размера эффекта. Вот я и решил разобраться в том, какие этапы развития мысли мы прошли на пути к современным рандомизированным экспериментам, чем и поделюсь в этом эссе.  

Прежде чем рассматривать историю AB, приведу небольшой рекап по тому, что это такое и зачем надо. Пожалуй, в любой сфере полезно понимать какие факторы на что и как сильно влияют. В терминах исследований это звучит как-то так: мы хотим знать насколько изменится показатель X у подопытного, если он получит некоторое воздействие T, в сравнении с состоянием, когда он это воздействие не получает. Подопытным, метрикой и воздействием, может быть что угодно, но основная идея сохранится в духе того, что написано выше. 

Вроде все легко, можно взять двух похожих подопытных, на одного влиять, другого не трогать и снять нужное нам измерение. Но это не сработает, потому что в мире слишком много факторов, которые мы не можем контролировать, чтобы считать измерения двух разных объектов или в разное время – сравнимыми. Получается, основная проблема на пути  к нашей цели, в том, что один и тот же объект не может одновременно быть в двух разных состояниях. Это называется “фундаментальная проблема причинно-следственного анализа”. 

Для закрепления приведу табличку, которая некогда мне очень помогла: 

Пусть X – подопытный объект, T{0/1} – индикатор того, было ли на объект воздействие, Y – некоторая метрика, которую мы хотим оценить, тогда мы для оценки эффекта мы должны заполнить такую табличку:



| Индекс подопытного $X_i$ | Значение, если было влияние $Y|T=1$ | Значение, если влияния не было $Y|T=0$ | Эффект от влияния |
|---------|:-----|------:|:------:|
| 1      | 1     |  ???  | 1 - ???|
| 2      | ???   |  2    | ??? - 2|
| 3      | ???   |  1.4  | ??? - 1.4|


Так проблему видно “в лицо”, ведь мы хотим получить разницу между тем когда есть воздействие и тем, когда его нет, но у нас не хватает данных. По сути, история развития экспериментов это история попыток как можно точнее оценивать пропущенные значения. Основные вехи в решении этой проблемы и конкретные переломные моменты я и попробую описать ниже, структурно я поделил историю вопроса на 5 этапов: 

* “наивные эксперименты”  
* рандомизация   
* измеряем случайность   
* улучшаем случайность   
* прогнозная оптимизация    

Теперь разберем каждый подробнее. 

**Этап 1 – “наивные эксперименты”**

Тут особо нечего говорить о каких-то систематических достижениях в области тестов, потому что их, по сути, и не было. Люди просто наблюдали явления, описывали их и наблюдаемое брали за оценку эффекта. Эффективность таких исследований низкая, потому что понятным образом отделить, где результат полностью случайный, а где нет – нельзя. 

С философской точки зрения хочется сказать, что смысл в таких экспериментах все равно был, просто валидация результата была не через контролируемую среду эксперимента, а через реальность, поэтому через время сохранялись наиболее соответствующие реальности результаты. 

Приведу несколько занимательных фактов и идей, времен этого периода: 

* Начала теории вероятности можно встретить в работах древних греков в 5 веке до нашей эры. Фукидид описывает как афиняне посчитали с с помощью наблюдений нескольких солдат высоту вражеской стены. 
* Описанные подходы к вычислению арифметического среднего и медианы появились только в 16 веке. 
* Ньютон в своих началах (17 век) для доказательств использует просто описание увиденных явлений (по сути во всех науках так было до изобретения статистических тестов, но этот пример меня особенно поразил) 
* Термин “статистика” – появился в 17 веке и долгое время в задачи статистики входил только сбор, систематизация и описание социально-демографических данных. 

**Этап 2 – рандомизация** 

На этой стадии ученые в два захода научились убирать индивидуальные особенности участников эксперимента. 

Первая проблема – личная предвзятость. Думаю не секрет, что люди любят свои гипотезы и хотят в них верить сильнее чем стоило бы, поэтому часто выбирают решения в пользу своих убеждений, что касается и экспериментаторов, и подопытных. Как избавиться от предвзятости, если переключать мышление мы не умеем? – убрать у знание о том, данные о чем конкретно мы анализируем.

Так появилась концепция слепых тестов, а первое зафиксированное применение датируется 1784 в исследовании магнетизма у животных. Суть в том, что мы стороны эксперимента не должны знать кто получает воздействие, а кто нет, чтобы не было соблазна искать в данных подтверждение своим мыслям, в ущерб реальности. 

Вторая проблема – как избавиться от индивидуальных особенностей единиц эксперимента. В 18-19 веках появляются значимые открытия в области статистики, а именно:

* В 1770-х Лапласс развил идею Арбутнота и впервые использовал что-то похожее на фреймворк статистической проверки гипотез, где мы выделяем 2 варианта – нулевую и альтернативную гипотезу и интерпретируем данные в пользу одной из них. Так появилась идея взаимно исключающих, но коллективно исчерпывающих сравнений. 
* С 1730-х по 1812 годы, Муавр, Гаусс и Лапласс формализуют и исследуют нормальное распределение и затем замечают, что биномиальное распределение в пределе стремится к нормальному. Так в 1812 году появляется первый частный случай центральной предельной теоремы, после чего активно будут появляться все новые улучшения, до того, как ЦПТ в общем виде не будет доказана в середине прошлого века (к слову, хороший [видос об истории ЦПТ](https://www.youtube.com/watch?v=XTCjKGK1WeM)). Это позволило мыслить  сторону того, что достаточно большое количество случайных наблюдений нейтрализует индивидуальные особенности каждого наблюдения. 

И какое это все имеет практическое отношение к AB тестам? А то, что эти три идеи говорят, что если мы зафиксируем какие-то ожидания и возьмем достаточно много случайных наблюдений, то сможем оценить эффект от воздействия. Так, в 1880-х годах, Чарльз Пирс в серии статей предлагает нечто, очень похожее на рандомизацию, которая, при достаточном числе наблюдений, позволяет считать, что индивидуальные особенности и прочие факторы среды нивелируются случайностью разделения подопытных на группы. 

**Этап 3 – измеряем случайность** 

Имея на руках базовые инструменты, ученые начали активно развивать свои подходы и в конце 19 – первой половине 20 века сформулировали основные идеи современных тестов. 

В это время формируются основные, популярные сегодня, подходы к формированию статистического вывода: 

* Байесовский – применимость байесовского статистического подхода к принятию решений доказал Абрахам Вальд. В данном подходе используются априорные знания, которые формируют некоторую статистическую модель и затем эта модель обновляется согласно наблюдаемым данным с помощью теоремы Байеса. 
* Частотный – первично его развивали Рональд Фишер, Джерси Нейман и Эгон Пирсон. Этот подход рассматривает наблюдаемые частоты как вероятности и переносит выводы из анализа наблюдаемых частот. 
* Правдоподобие – изначально эта школа не была отдельной и развивалась в рамках подхода Фишера к статистике, который предлагал метод максимизации правдоподобия. Позже методики развились в отдельное течение и первое систематическое, самостоятельное использование методов анализа правдоподобия можно найти у Энтони Эдвардса. Эта парадигма предполагает подбирать параметры статистической модели на основе наблюдаемых данных для оптимизации функции правдоподобия. 

Также, это было время настоящих суперзвезд статистики, вот даты открытия самых известных фактов: 

* 1904 Карл Пирсон разрабатывает тест – Хи-квадрат
* 1908 Уильям Госсет разрабатывать t-тест 
* 1918 Фишер формализует понятие дисперсии
* 1923 Джерси Нейман описывает применимость рандомизации к проведению статистических экспериментов и предлагает фреймворк причинно-следственного анализа
* 1937 Джерси Нейман определил концепцию доверительных интервалов. 
* 1935 Фишер формализует подходы к дизайну статистических экспериментов 
* 1958 Кокс формулирует главную предпосылку экспериментов – SUTVA – требование о том, что каждый подопытный должен получать только один тип воздействия (или его отсутствие)
* В 1974-1980 Дональд Рубин в серии статей расширяет фреймворк, предложенный Нейманом, который сейчас известен как rubin causal model и теперь является стандартом в вопросе причинно следственного анализа. 
* К середине 20 века у человечества уже были описаны все основные концепции, которые сейчас используются в AB тестах. Люди умели случайным образом составлять группы, оценивать параметры выборки и делать вывод о том, насколько наблюдаемые значения случайны. 

**Этап 4 – улучшаем случайность** 

Казалось бы, вот уже все умеем считать и делать вывод о случайности или неслучайности явлений, что дальше? На самом деле, люди быстро столкнулись с проблемой, что хочется проверять влияние как можно более точечных факторов, а данных на всех не хватает, поэтому надо уметь получать достоверные результаты, с наименьшим количеством данных. 

Изначально, теория вероятности, а особенно ЦПТ, сказали, что если мы соберем достаточно много наблюдений, то можем пренебречь факторами, которые влияют на каждое индивидуальное наблюдение. Теперь же ученые решили, что могут использовать известные факторы в своих целях. Основная идея следующая – вместо того, чтобы отбрасывать знание об испытуемых с помощью больших выборок, мы можем использовать это знание, чтобы сравнивать подобное с подобным. Для себя я называл это “улучшение случайности”, потому что люди стали придумывать способы использовать знания о случайных наблюдениях для улучшения своих статистических оценок параметров. Тут была проделана огромная работа, опишу лишь базовые и самые популярные методы: 

* 1960 Дональд Кампбел предлагает метод regression discontinuity design – где мы анализируем наблюдения в окрестности предполагаемого влияния
* 1983 Дональд рубин предлагает технику propensity  score matching – которая предполагает изменять значимость наблюдений согласно их популярности в данных
* 1984 Орли Эшенфельтер и Дэвид Кард разрабатывают метод Diff-in-Diff, где мы используем данные о похожих наблюдениях для прогноза значений в целевых.  
* 2000 Джуда Перл дал полное формальное определение инструментальных переменных – техники использования внешней независимой переменной для исключения влияния неизвестных факторов

Отмечу, что это далеко не исчерпывающий список, но, как мне показалось, он хорошо показа идею, что люди стали активно улучшать методы выбора групп для сравнения. Также, обязательно надо отметить, что все техники ML стали активно использоваться для улучшения экспериментов, основных направления для использования – два: снизить дисперсию (т.е. сделать оценку более устойчивой) и убрать смещение (т.е. Сделать оценку более приближенной к реальности). 

По-моему мнению, мы находимся на данном этапе. Сейчас мы умеем чудесно проводить реальные эксперименты и организовывать синтетические (такие, где результаты контрольной группы прогнозируются, а не берутся напрямую из эксперимента), но что же далее? 

**Этап 5 – прогнозная оптимизация**

Это моя попытка угадать как дальше будут развиваться эксперименты. Напомню какой путь человечество прошло в плане экспериментов за последние 300 лет: 

* Экспериментов нет – выводы о причинах и следствиях делаются из личного опыта и убеждений 
* Классические эксперименты – чтобы сделать вывод, надо разделить большое количество подопытных по группам случайным образом и использовать статистические процедуры для оценки эффекта и его значимости 
* Прогнозные эксперименты – можно оценивать эффект от воздействия прогнозируя поведение контрольной группы. 

В такой цепочке мне кажется абсолютно логичным следующий шаг – научиться оценивать возможные эффекты без осуществления какого-либо воздействия, т.е. чтобы сказать, что событие X меняет показатель Y на N% не нужно будет ни на кого влиять этим событием. Если это случится, то задачи оптимизации любых явлений станут делом техники. 

Рассмотрим на примере бизнеса – сейчас на старте предприниматель выбирает некоторый набор гипотез и проверяет и стремится как можно быстрее их проверить, создавая бизнес модель, продукт и т.п, чтобы понять, что из этого сработает. В парадигме, где существует возможность оценить любой эффект без какого-либо эксперимента, предприниматель сможет описать параметры гипотезы, которую он хочет проверить и в ответе получит ожидаемый размер прибыли. И если такой механизм станет исправно существовать, то очень скоро появится другой, который будет генерировать вопросы и находить оптимальные решения, согласно прогнозам этой модели. 

По сути, это упражнение уже сейчас делают много где, в том же бизнесе люди создают прогнозные бизнес-модели, ученые выдвигают гипотезы о каких-либо явлениях и т.п., но относительно простого механизма получить прогноз о сложной системе по заданным параметрам у нас сейчас нет. 

Чтобы мое предложение “всезнающего черного ящика” не казалось слишком абсурдным, приведу примеры, где нечто-похожее уже используют: 

* [Ребята из гарварда](https://www.hbs.edu/ris/Publication%2520Files/23-062_b8fbedcd-ade4-49d6-8bb7-d216650ff3bd.pdf) исследовали можно ли использовать chat-gpt для сбора данных о предпочтениях пользователей. По итогу, модель вполне успешно симулирует ответы реальных потребителей. 
* Большая компашка по исследованиям рынка [пишет](https://kadence.com/how-large-language-models-are-changing-market-research/), как LLM помогают им с проведением опросов. Если коротко, то на каждом этапе польза есть, от составления опросника, до имитации ответов и очистки данных. 
* [Приложение](https://on-page.ai/pages/auto-optimize/), которое само оптимизирует SEO. LLM читает текст, генерирует ключевые слова и проверяет лучшие через поисковой движков, если повторить это несколько раз, то будут подобраны годные ключевые слова. 
* [Подборка](https://blog.evolv.ai/enhancing-ux-design-process-ai), стартапов, которые улучшают UI с помощью прогнозов по пользовательскому поведению от моделек. 
* [Sales ninja](https://sales-ninja.me/personalizations) – ребята делают путь пользователя персональным